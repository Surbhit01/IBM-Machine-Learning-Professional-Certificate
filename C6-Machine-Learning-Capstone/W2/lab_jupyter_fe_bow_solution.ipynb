{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extract Bag of Words (BoW) Features from Course Textual Content**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of recommender systems is to help users find items they potentially interested in. Depending on the recommendation tasks, an item can be a movie, a restaurant, or, in our case, an online course. \n",
    "\n",
    "Machine learning algorithms cannot work on an item directly so we first need to extract features and represent the items mathematically, i.e., with a feature vector.\n",
    "\n",
    "Many items are often described by text so they are associated with textual data, such as the titles and descriptions of a movie or course. Since machine learning algorithms can not process textual data directly, we need to transform the raw text into numeric feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/extract_textual_features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be learning to extract the bag of words (BoW) features from course titles and descriptions. The BoW feature is a simple but effective feature characterizing textual data and is widely used in many textual machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract Bag of Words (BoW) features from course titles and descriptions\n",
    "* Build a course BoW dataset to be used for building a content-based recommender system later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and setup the lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.6.7\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from nltk==3.6.7) (8.1.3)\n",
      "Collecting joblib (from nltk==3.6.7)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3 (from nltk==3.6.7)\n",
      "  Downloading regex-2024.4.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.6/761.6 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from nltk==3.6.7) (4.60.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from click->nltk==3.6.7) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.7) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.7) (4.5.0)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.3.2 nltk-3.6.7 regex-2024.4.16\n",
      "Collecting gensim==4.1.2\n",
      "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gensim==4.1.2) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gensim==4.1.2) (1.7.3)\n",
      "Collecting smart-open>=1.8.1 (from gensim==4.1.2)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim==4.1.2) (1.14.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-7.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.7\n",
    "!pip install gensim==4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW features are essentially the counts or frequencies of each word that appears in a text (string). Let's illustrate it with some simple examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two course descriptions as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "course1 = \"this is an introduction data science course which introduces data science to beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "course2 = \"machine learning for beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an introduction data science course which introduces data science to beginners',\n",
       " 'machine learning for beginners']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = [course1, course2]\n",
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the two strings into words (tokens). A token in the text processing context means the smallest unit of text such as a word, a symbol/punctuation, or a phrase, etc. The process to transform a string into a collection of tokens is called `tokenization`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common way to do ```tokenization``` is to use the Python built-in `split()` method of the `str` class.  However, in this lab, we want to leverage the `nltk` (Natural Language Toolkit) package, which is probably the most commonly used package to process text or natural language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " More specifically, we will use the ```word_tokenize()``` method on the content of course (string):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the two courses\n",
    "tokenized_courses = [word_tokenize(course) for course in courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the cell output, two courses have been tokenized and turned into two token arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a token dictionary to index all tokens. Basically, we want to assign a key/index for each token. One way to index tokens is to use the `gensim` package which is another popular package for processing textual data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a token dictionary for the two courses\n",
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'beginners': 1, 'course': 2, 'data': 3, 'introduces': 4, 'introduction': 5, 'is': 6, 'science': 7, 'this': 8, 'to': 9, 'which': 10, 'for': 11, 'learning': 12, 'machine': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokens_dict.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the token dictionary, we can easily count each token in the two example courses and output two BoW feature vectors. However, more conveniently, the `gensim` package provides us a `doc2bow` method to generate BoW features out-of-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate BoW features for each course\n",
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 2),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1)],\n",
       " [(1, 1), (11, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs two BoW arrays where each element is a tuple, e.g., (0, 1) and (7, 2). The first element of the tuple is the token ID and the second element is its count. So `(0, 1)` means `(``an``, 1)` and `(7, 2)` means `(``science``, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following code snippet to print each token and its count:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words for course 0:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 1:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n"
     ]
    }
   ],
   "source": [
    "# Enumerate through each course and its bag-of-words representation\n",
    "for course_idx, course_bow in enumerate(courses_bow):\n",
    "    # Print the index of the current course and a label\n",
    "    print(f\"Bag of words for course {course_idx}:\")\n",
    "    # For each token index, print its bow value (word count)\n",
    "    for token_index, token_bow in course_bow:\n",
    "        # Retrieve the token from the tokens dictionary based on its index\n",
    "        token = tokens_dict.get(token_index)\n",
    "        # Print the token and its bag-of-words value\n",
    "        print(f\"--Token: '{token}', Count:{token_bow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we turn to the long list into a horizontal feature vectors, we can see the two courses become two numerical feature vectors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/bow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document may contain tens of thousands of words which makes the dimension of the BoW feature vector huge. To reduce the dimensionality, one common way is to filter the relatively meaningless tokens such as stop words or sometimes add position and adjective words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there are many other ways to reduce dimensionality such as `stemming` and `lemmatization` but they are beyond the scope of this capstone project. You are encouraged to explore them yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the english stop words provided in `nltk`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can filter those English stop words from the tokens in course1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'which',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'to',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens in course 1\n",
    "tokenized_courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_tokens = [w for w in tokenized_courses[0] if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the number of tokens for ```course1``` has been reduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way is to only keep nouns in the text. We can use the `nltk.pos_tag()` method to analyze the part of speech (POS) and annotate each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('introduction', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('course', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('introduces', 'VBZ'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('beginners', 'NNS')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_courses[0])\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see [`introduction`, `data`, `science`, `course`, `beginners`] are all of the nouns and we may keep them in the BoW feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Extract BoW features for course textual content and build a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you have learned what a BoW feature is, so let's start extracting BoW features from some real course textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "course_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv\"\n",
    "course_content_df = pd.read_csv(course_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COURSE_ID                                               ML0201EN\n",
       "TITLE          robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION    have fun with iot and learn along the way  if ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course content dataset has three columns `COURSE_ID`, `TITLE`, and `DESCRIPTION`. `TITLE` and `DESCRIPTION` are all text upon which we want to extract BoW features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join those two text columns together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge TITLE and DESCRIPTION title\n",
    "course_content_df['course_texts'] = course_content_df[['TITLE', 'DESCRIPTION']].agg(' '.join, axis=1)\n",
    "course_content_df = course_content_df.reset_index()\n",
    "course_content_df['index'] = course_content_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                           0\n",
       "COURSE_ID                                                ML0201EN\n",
       "TITLE           robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION     have fun with iot and learn along the way  if ...\n",
       "course_texts    robots are coming  build iot apps with watson ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have prepared a `tokenize_course()` method for you to tokenize the course content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_course(course, keep_only_nouns=True):\n",
    "    # Get English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the course text\n",
    "    word_tokens = word_tokenize(course)\n",
    "    # Remove English stop words and numbers\n",
    "    word_tokens = [w for w in word_tokens if (not w.lower() in stop_words) and (not w.isnumeric())]\n",
    "    # Only keep nouns \n",
    "    if keep_only_nouns:\n",
    "        # Define a filter list of non-noun POS tags\n",
    "        filter_list = ['WDT', 'WP', 'WRB', 'FW', 'IN', 'JJR', 'JJS', 'MD', 'PDT', 'POS', 'PRP', 'RB', 'RBR', 'RBS',\n",
    "                       'RP']\n",
    "        # Tag the word tokens with POS tags\n",
    "        tags = nltk.pos_tag(word_tokens)\n",
    "        # Filter out non-nouns based on POS tags\n",
    "        word_tokens = [word for word, pos in tags if pos not in filter_list]\n",
    "\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the first course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robots are coming  build iot apps with watson  swift  and node red have fun with iot and learn along the way  if you re a swift developer and want to learn more about iot and watson ai services in the cloud  raspberry pi   and node red  you ve found the right place  you ll build iot apps to read temperature data  take pictures with a raspcam  use ai to recognize the objects in those pictures  and program an irobot create 2 robot  '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_course = course_content_df.iloc[0, :]['course_texts']\n",
    "a_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robots',\n",
       " 'coming',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'watson',\n",
       " 'swift',\n",
       " 'red',\n",
       " 'fun',\n",
       " 'iot',\n",
       " 'learn',\n",
       " 'way',\n",
       " 'swift',\n",
       " 'developer',\n",
       " 'want',\n",
       " 'learn',\n",
       " 'iot',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'services',\n",
       " 'cloud',\n",
       " 'raspberry',\n",
       " 'pi',\n",
       " 'node',\n",
       " 'red',\n",
       " 'found',\n",
       " 'place',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'read',\n",
       " 'temperature',\n",
       " 'data',\n",
       " 'take',\n",
       " 'pictures',\n",
       " 'raspcam',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'recognize',\n",
       " 'objects',\n",
       " 'pictures',\n",
       " 'program',\n",
       " 'irobot',\n",
       " 'create',\n",
       " 'robot']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_course(a_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need to write some code snippets to generate the BoW features for each course. Let's start by tokenzing all courses in the `courses_df`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Use provided tokenize_course() method to tokenize all courses in courses_df['course_texts']._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "a_course = course_content_df['course_texts'].to_list()\n",
    "a_course_all = ' '.join(a_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "\n",
    "Use `tokenize_course(text, True)` command to tokenize each text in `courses_df['course_texts']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create a token dictionary `tokens_dict`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `doc2bow()` method to generate BoW features for each tokenized course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "tokenized_courses = [word_tokenize(course) for course in a_course]\n",
    "\n",
    "a_courses = gensim.corpora.Dictionary(tokenized_courses)\n",
    "# Generate BoW features for each course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Use gensim.corpora.Dictionary(tokenized_courses) to create a token dictionary._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Use tokens_dict.doc2bow() to generate BoW features for each tokenized course._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "    \n",
    "You can use `tokens_dict.doc2bow(course)` command  for each course in `tokenized_courses`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you need to append the BoW features for each course into a new BoW dataframe. The new dataframe needs to include the following columns (you may include other relevant columns as well):\n",
    "- 'doc_index': the course index starting from 0\n",
    "- 'doc_id': the actual course id such as `ML0201EN`\n",
    "- 'token': the tokens for each course\n",
    "- 'bow': the bow value for each token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Create a new course_bow dataframe based on the extracted BoW features._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words for course 0:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 1:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:6\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 2:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 3:\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 4:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 5:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 6:\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 7:\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 8:\n",
      "--Token: 'course', Count:1\n",
      "Bag of words for course 9:\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 10:\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 11:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 12:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:4\n",
      "Bag of words for course 13:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 14:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 15:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "Bag of words for course 16:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 17:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 18:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:7\n",
      "--Token: 'is', Count:4\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:9\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 19:\n",
      "Bag of words for course 20:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "Bag of words for course 21:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 22:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 23:\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 24:\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 25:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 26:\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 27:\n",
      "Bag of words for course 28:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 29:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 30:\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 31:\n",
      "--Token: 'course', Count:1\n",
      "Bag of words for course 32:\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 33:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 34:\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 35:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:5\n",
      "Bag of words for course 36:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 37:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 38:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 39:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 40:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:5\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 41:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 42:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:4\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 43:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 44:\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 45:\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'science', Count:2\n",
      "Bag of words for course 46:\n",
      "--Token: 'an', Count:1\n",
      "Bag of words for course 47:\n",
      "Bag of words for course 48:\n",
      "Bag of words for course 49:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 50:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 51:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 52:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 53:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 54:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 55:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 56:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 57:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "Bag of words for course 58:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 59:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 60:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:2\n",
      "Bag of words for course 61:\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 62:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 63:\n",
      "Bag of words for course 64:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 65:\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 66:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 67:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 68:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 69:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 70:\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 71:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 72:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 73:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 74:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 75:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 76:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 77:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 78:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 79:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 80:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 81:\n",
      "Bag of words for course 82:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 83:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 84:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 85:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 86:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 87:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 88:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 89:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 90:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 91:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 92:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 93:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 94:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 95:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 96:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 97:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 98:\n",
      "Bag of words for course 99:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 100:\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 101:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 102:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 103:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 104:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 105:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 106:\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 107:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 108:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 109:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 110:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "Bag of words for course 111:\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 112:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 113:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 114:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 115:\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:6\n",
      "Bag of words for course 116:\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 117:\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 118:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'learning', Count:3\n",
      "Bag of words for course 119:\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 120:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 121:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 122:\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 123:\n",
      "Bag of words for course 124:\n",
      "Bag of words for course 125:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 126:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 127:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 128:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 129:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 130:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:2\n",
      "Bag of words for course 131:\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'science', Count:2\n",
      "Bag of words for course 132:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 133:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 134:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 135:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'which', Count:2\n",
      "Bag of words for course 136:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 137:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 138:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 139:\n",
      "Bag of words for course 140:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 141:\n",
      "Bag of words for course 142:\n",
      "--Token: 'learning', Count:2\n",
      "Bag of words for course 143:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 144:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 145:\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 146:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:7\n",
      "Bag of words for course 147:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 148:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 149:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 150:\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 151:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'learning', Count:3\n",
      "Bag of words for course 152:\n",
      "Bag of words for course 153:\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 154:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 155:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 156:\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 157:\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 158:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:3\n",
      "Bag of words for course 159:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 160:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 161:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 162:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 163:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 164:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 165:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 166:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 167:\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 168:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 169:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 170:\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 171:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 172:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 173:\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 174:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 175:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 176:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:7\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:5\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 177:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 178:\n",
      "--Token: 'data', Count:2\n",
      "Bag of words for course 179:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 180:\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 181:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 182:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 183:\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:6\n",
      "Bag of words for course 184:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 185:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 186:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 187:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 188:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 189:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 190:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 191:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 192:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 193:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 194:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 195:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 196:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 197:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 198:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:6\n",
      "Bag of words for course 199:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:3\n",
      "Bag of words for course 200:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'learning', Count:5\n",
      "--Token: 'machine', Count:4\n",
      "Bag of words for course 201:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 202:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "Bag of words for course 203:\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 204:\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 205:\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "Bag of words for course 206:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 207:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 208:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 209:\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 210:\n",
      "--Token: 'to', Count:2\n",
      "Bag of words for course 211:\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 212:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "Bag of words for course 213:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 214:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:7\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 215:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 216:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:7\n",
      "Bag of words for course 217:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:17\n",
      "--Token: 'is', Count:5\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:18\n",
      "--Token: 'for', Count:8\n",
      "Bag of words for course 218:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:12\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'for', Count:4\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 219:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:8\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:10\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 220:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'for', Count:5\n",
      "Bag of words for course 221:\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 222:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:13\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:10\n",
      "--Token: 'for', Count:4\n",
      "Bag of words for course 223:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 224:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 225:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 226:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:7\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'for', Count:5\n",
      "Bag of words for course 227:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 228:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:8\n",
      "Bag of words for course 229:\n",
      "--Token: 'course', Count:6\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 230:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'which', Count:2\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 231:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:5\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 232:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 233:\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 234:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:3\n",
      "--Token: 'machine', Count:3\n",
      "Bag of words for course 235:\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:8\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 236:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:9\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 237:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:7\n",
      "--Token: 'for', Count:4\n",
      "Bag of words for course 238:\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'to', Count:1\n",
      "Bag of words for course 239:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 240:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 241:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 242:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 243:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:7\n",
      "Bag of words for course 244:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:13\n",
      "--Token: 'introduces', Count:3\n",
      "--Token: 'is', Count:4\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 245:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:26\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'this', Count:7\n",
      "--Token: 'to', Count:13\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 246:\n",
      "--Token: 'an', Count:3\n",
      "--Token: 'course', Count:7\n",
      "--Token: 'data', Count:22\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'is', Count:6\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:7\n",
      "--Token: 'to', Count:15\n",
      "--Token: 'for', Count:10\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 247:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 248:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:6\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:14\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 249:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:9\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 250:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:7\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:11\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 251:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:12\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:9\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 252:\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:9\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:15\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:7\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 253:\n",
      "--Token: 'course', Count:6\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:6\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:4\n",
      "Bag of words for course 254:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:6\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:7\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 255:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 256:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 257:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:6\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:7\n",
      "--Token: 'to', Count:11\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 258:\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:5\n",
      "Bag of words for course 259:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:9\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:14\n",
      "--Token: 'machine', Count:9\n",
      "Bag of words for course 260:\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:10\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:7\n",
      "--Token: 'to', Count:19\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:17\n",
      "--Token: 'machine', Count:17\n",
      "Bag of words for course 261:\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:4\n",
      "Bag of words for course 262:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:3\n",
      "--Token: 'machine', Count:3\n",
      "Bag of words for course 263:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'science', Count:5\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:9\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 264:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:6\n",
      "--Token: 'machine', Count:5\n",
      "Bag of words for course 265:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:8\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:6\n",
      "--Token: 'machine', Count:5\n",
      "Bag of words for course 266:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:7\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:6\n",
      "--Token: 'machine', Count:5\n",
      "Bag of words for course 267:\n",
      "--Token: 'course', Count:6\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:6\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:4\n",
      "Bag of words for course 268:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 269:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:6\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:5\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 270:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 271:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:4\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 272:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:4\n",
      "--Token: 'to', Count:10\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 273:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:9\n",
      "--Token: 'for', Count:3\n",
      "--Token: 'learning', Count:9\n",
      "--Token: 'machine', Count:5\n",
      "Bag of words for course 274:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:13\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:8\n",
      "--Token: 'machine', Count:4\n",
      "Bag of words for course 275:\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:8\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 276:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:13\n",
      "--Token: 'is', Count:4\n",
      "--Token: 'science', Count:10\n",
      "--Token: 'this', Count:6\n",
      "--Token: 'to', Count:11\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 277:\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:15\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:4\n",
      "--Token: 'this', Count:4\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'for', Count:5\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 278:\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:11\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'science', Count:7\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:12\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 279:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:4\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 280:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:19\n",
      "--Token: 'introduction', Count:2\n",
      "--Token: 'is', Count:6\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:13\n",
      "--Token: 'for', Count:6\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 281:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:30\n",
      "--Token: 'is', Count:4\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:4\n",
      "--Token: 'to', Count:14\n",
      "--Token: 'for', Count:5\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 282:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:10\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:11\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:7\n",
      "--Token: 'machine', Count:7\n",
      "Bag of words for course 283:\n",
      "--Token: 'data', Count:9\n",
      "--Token: 'science', Count:1\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:6\n",
      "--Token: 'for', Count:2\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 284:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 285:\n",
      "--Token: 'an', Count:4\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:10\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:5\n",
      "--Token: 'machine', Count:1\n",
      "Bag of words for course 286:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 287:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:9\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:4\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 288:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:5\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 289:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'for', Count:4\n",
      "Bag of words for course 290:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'learning', Count:5\n",
      "--Token: 'machine', Count:2\n",
      "Bag of words for course 291:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "Bag of words for course 292:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 293:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 294:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:7\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:5\n",
      "--Token: 'to', Count:11\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 295:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:7\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:6\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:7\n",
      "Bag of words for course 296:\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'data', Count:4\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:2\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 297:\n",
      "--Token: 'an', Count:2\n",
      "--Token: 'course', Count:7\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'for', Count:4\n",
      "--Token: 'learning', Count:4\n",
      "--Token: 'machine', Count:4\n",
      "Bag of words for course 298:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:4\n",
      "--Token: 'data', Count:8\n",
      "--Token: 'is', Count:6\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'which', Count:1\n",
      "--Token: 'for', Count:5\n",
      "--Token: 'learning', Count:1\n",
      "Bag of words for course 299:\n",
      "--Token: 'course', Count:5\n",
      "--Token: 'data', Count:6\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'science', Count:3\n",
      "--Token: 'this', Count:3\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 300:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:13\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:2\n",
      "Bag of words for course 301:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:4\n",
      "--Token: 'for', Count:1\n",
      "Bag of words for course 302:\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:3\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'which', Count:2\n",
      "Bag of words for course 303:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:5\n",
      "--Token: 'for', Count:2\n",
      "Bag of words for course 304:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:6\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:4\n",
      "--Token: 'to', Count:13\n",
      "--Token: 'for', Count:5\n",
      "Bag of words for course 305:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:2\n",
      "--Token: 'this', Count:2\n",
      "--Token: 'to', Count:8\n",
      "--Token: 'for', Count:3\n",
      "Bag of words for course 306:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:3\n",
      "--Token: 'data', Count:3\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:3\n",
      "--Token: 'for', Count:2\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "# Enumerate through each course and its bag-of-words representation\n",
    "for course_idx, course_bow in enumerate(courses_bow):\n",
    "    # Print the index of the current course and a label\n",
    "    print(f\"Bag of words for course {course_idx}:\")\n",
    "    # For each token index, print its bow value (word count)\n",
    "    for token_index, token_bow in course_bow:\n",
    "        # Retrieve the token from the tokens dictionary based on its index\n",
    "        token = tokens_dict.get(token_index)\n",
    "        # Print the token and its bag-of-words value\n",
    "        print(f\"--Token: '{token}', Count:{token_bow}\")\n",
    "\n",
    "#  ...\n",
    "#  bow_dicts = {\"doc_index\": doc_indices,\n",
    "#            \"doc_id\": doc_ids,\n",
    "#            \"token\": tokens,\n",
    "#            \"bow\": bow_values}\n",
    "#  pd.DataFrame(bow_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "    \n",
    "You can use 2 for-loops to create your data frame: first one will be `for doc_index, doc_bow in enumerate(bow_docs):` where bow_docs is the list of BoW features for each tokenized course and within this for-loop you will have another loop `for token_index, token_bow in doc_bow:`. Then you can get each \"token\" by applying the `token_index` to your `token_dict`,  `token_bow` will give you \"bow\" values, `doc_indices` will give you values for  \"doc_index\" and you can get \"doc_id\" by using `courses_df['COURSE_ID']` list and `doc_index` as indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your course BoW dataframe may look like the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/bow_dataset.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to previous code examples in this lab if you need help with creating the BoW dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other popular textual features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic token BoW feature, there are two other types of widely used textual features. If you are interested, you may explore them yourself to learn how to extract them from the course textual content: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf-idf**: tf-idf refers to Term Frequency–Inverse Document Frequency. Similar to BoW, the tf-idf also counts the word frequencies in each document. Furthermore, tf-idf will  offset the number of documents in the corpus that contain the word in order to adjust for the fact that some words appear more frequently in general. The higher the tf-idf normally means the greater the importance the word/token is.\n",
    "- **Text embedding vector**. Embedding means projecting an object into a latent feature space. We normally employ neural networks or deep neural networks to learn the latent features of a textual object such as a word, a sentence, or the entire document. The learned latent feature vectors will be used to represent the original textual entities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have completed the BoW feature extraction lab. In this lab, you have learned and practiced extracting BoW features from course titles and descriptions. Once the feature vectors on the courses has been built, we can then apply machine learning algorithms such as similarity measurements, clustering, or classification on the courses in later labs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle## Change Log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "```\n",
    "```toggle|-|-|-|-|\n",
    "```\n",
    "```toggle|2021-10-25|1.0|Yan|Created the initial version|\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "8a26cc99127d60eeaa7ab3d73ccfdcf6658faecdb16a63a229b1534af28b0a1d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
